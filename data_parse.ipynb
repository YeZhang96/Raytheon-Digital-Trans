{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CSX.json', 'ADI_News.json', 'AMETEKInc.json', 'CabotOG.json', 'grainger.json', 'dish.json', 'cardinalhealth.json', 'digitalrealty.json', 'BNYMellon.json', 'HelmerichPayne.json', 'citrix.json', 'Ford.json', 'Garmin.json', 'Ecolab.json', 'HiltonHotels.json', 'Gap.json', 'HersheyCompany.json', '3M.json', 'BDandCo.json', 'Centene.json', 'FBHS_News.json', 'CapitalOne.json', 'Hologic.json', 'bakerhughesco.json', 'bmsnews.json', 'FastenalCompany.json', 'CrownCastle.json', 'BestBuy.json', 'eBay.json', 'Fiserv.json', 'Accenture.json', 'GlobeLife.json', 'comcast.json', 'extraspace.json', 'CampbellSoupCo.json', 'Chevron.json', 'Fortinet.json', 'BorgWarner.json', 'HenrySchein.json', 'AmericanAir.json', 'GM.json', 'bostonsci.json', 'Boeing.json', 'Citi.json', 'GoldmanSachs.json', 'CFGCommercial.json', 'DRHorton.json', 'DXCTechnology.json', 'AristaNetworks.json', 'CDWCorp.json', 'eatoncorp.json', 'blackrock.json', 'DentsplySirona.json', 'BankofAmerica.json', 'EdwardsLifesci.json', 'CBRE.json', 'Cognizant.json', 'AimcoApts.json', 'AdvanceAuto.json', 'CintasCorp.json', 'atmosenergy.json', 'ComericaBank.json', 'FifthThird.json', 'FTI_US.json', 'CenturyLink.json', 'ChipotleTweets.json', 'askRegions.json', 'HCAhealthcare.json', 'facebook.json', 'GallagherGlobal.json', 'amwater.json', 'Cisco.json', 'CHRobinson.json', 'Flowserve.json', 'CocaCola.json', 'AlphabetInc.json', 'DowNewsroom.json', 'EsteeLauder.json', 'ANSYS.json', 'BallCorpHQ.json', 'Exelon.json', 'DaVita.json', 'CarnivalPLC.json', 'baxter_intl.json', 'darden.json', 'CharlesSchwab.json', 'CBS.json', 'celanese.json', 'AvalonBay.json', 'HormelFoods.json', 'firstrepublic.json', 'DuPont_News.json', 'AIGinsurance.json', 'autozone.json', 'Delta.json', 'CarMax.json', 'exxonmobil.json', 'amazon.json', 'Healthcare_ABC.json', 'AmericanExpress.json', 'Cerner.json', 'conocophillips.json', 'flir.json', 'Chubb.json', 'biogen.json', 'AmerenCorp.json', 'FortiveCorp.json', 'AMD.json', 'BBT.json', 'Disney.json', 'DevonEnergy.json', 'Equifax.json', 'ConEdison.json', 'GetSpectrum.json', 'EXPD_Official.json', 'IFF.json', 'Celgene.json', 'generalelectric.json', 'AlaskaAir.json', 'AveryDennison.json', 'AbbottNews.json', 'Allstate.json', 'DominionEnergy.json', 'Adobe.json', 'energyinsights.json', 'ConagraBrands.json', 'CMEGroup.json', 'Halliburton.json', 'amcorpackaging.json', 'Aptiv.json', 'Equinix.json', 'Entergy.json', 'FISGlobal.json', 'F5Networks.json', '2K.json', 'FleetcorCareers.json', 'abiomedimpella.json', 'CaterpillarInc.json', 'AlbemarleCorp.json', 'Corning.json', 'Amgen.json', 'Applied4Tech.json', 'DiscoveryIncTV.json', 'Agilent.json', 'AnthemInc.json', 'AlexionPharma.json', 'ADMupdates.json', 'GlobalPayInc.json', 'harleydavidson.json', 'EssexProperties.json', 'DTE_Energy.json', 'Cadence.json', 'ATT.json', 'AllegionUS.json', 'AEPnews.json', 'DukeRealty.json', 'firstenergycorp.json', 'CVSHealth.json', 'aflac.json', 'FederalRealty.json', 'Hasbro.json', 'DollarTree.json', 'autodesk.json', 'FOXTV.json', 'AllianceData.json', 'abbvie.json', 'Broadridge.json', 'GeneralMills.json', 'Cummins.json', 'HIIndustries.json', 'edisonintl.json', 'Broadcom.json', 'etrade.json', 'Emerson_News.json', 'CruiseNorwegian.json', 'Gartner_inc.json', 'Akamai.json', 'CooperCompany.json', 'EA.json', 'honeywell.json', 'CP_News.json']\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "path = './SP-500-Twitter/'\n",
    "json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n",
    "print(json_files)\n",
    "print(len(json_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total twitts of CSX.json: 3244\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(path + 'eBay.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "print(\"total twitts of %s: %d\" %(json_files[0], len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help sellers like Marcel support the end of childhood cancer. When you like, reply to, or RT this post, we'll donate $1 to @StJude, up to $10k total. https://t.co/XAOcNd2QpD\n",
      "â€œThey say superheroes never die.â€ Today we lost one. RIP Stan Lee. https://t.co/thYgCb8Pg9\n",
      "We've got two exclusive signed guitars by @Harry_Styles and @NiallOfficial up for grabs, including an iconic piece from the Gibson custom shop. #GRAMMYs https://t.co/ykCQSjIVH3 https://t.co/6iuVYFsfOw\n",
      "Bid on this exclusive hooded stuffed gorilla signed by @TheEllenShow and @portiaderossi! 100% of funds go toward @TheEllenFund, an org that benefits global conservation efforts for endangered species. ðŸ¦ðŸ’š \n",
      "\n",
      "Bid now: https://t.co/xHLHEPWbZN https://t.co/wqWtcSGRdu\n",
      "Last chance to bid to meet @StephenCurry30 and own signed @warriors memorabila. 100% to @GSWfoundation: https://t.co/x48bj5LPGC https://t.co/s6uGgqj6P0\n",
      "Get your claws on an exclusive variant cover of @Marvelâ€™s new #BlackPanther comic book on eBay. Designed by @sanfordgreene and for a special price of $5.99. https://t.co/BJtMEQ0oYA https://t.co/Q65JOaRr4o\n",
      "That's right, zero selling fees on sneakers over $100. Happy listing! ðŸ¤‘ https://t.co/8KGjpR1wet\n",
      "Join @StephenCurry30 in his pregame routine &amp; toss the ball during his tunnel shot on 4/12! 100% to @GSWFoundation https://t.co/x48bj5LPGC https://t.co/aD6hQLoICm\n",
      "The music community is at the #GRAMMYs Person of the Year event. For @juanes, it's all about giving back to the community. Show your support to @MusiCares by bidding on one-of-a-kind items signed by your fav artists. https://t.co/pVud6WK999 https://t.co/q1VBlXa1Vp\n",
      "Here's your chance to win season tickets to the @Suns &amp; signed merchandise from @BonJovi. 100% to @JBJSoulFound: https://t.co/bPSN2a5An0 https://t.co/ryZa9gzE16\n",
      "Winter is coming. RT w/ #FirstMinuteShopping &amp; #Sweepstakes for a chance to win this North Face + other prizes. https://t.co/hNCIutiLMM https://t.co/xkC4YU9NDB\n",
      "Calling all female superhero fans! Tell us about your favorite superheroine for a chance to win a $100 gift card to shop the worldâ€™s first all-female superhero store, Superheroine HQ, now open on eBay. Be sure to tag us and include #mysuperhero and #sweepstakes!\n",
      "New season. New savings. Save 15% today on all of your spring needs. For more *details, visit: https://t.co/wdTR5rECo6 https://t.co/479VapiTdj\n",
      "RT w/ #FirstMinuteShopping &amp; #Sweepstakes for a chance to win a $100 eBay gift card this holiday season. https://t.co/hNCIutiLMM https://t.co/c24WQpyJpR\n",
      "Weâ€™re scouring our listings for you, too, @DaleJr. Weâ€™ve got your back. https://t.co/L0ezzNMddn\n",
      "For the one you thought would never go on sale...\n",
      "We â¤ï¸ hearing about the heartfelt moments between buyers &amp; sellers. You never know how your listing could impact someoneâ€™s life! Whatâ€™s the most meaningful experience youâ€™ve had on eBay? #ebayfinds https://t.co/Qbk0EngKDP\n",
      ".@AshvsEvilDead fans: Relive the hit TV show by snagging one of these original props used on set. https://t.co/qKMcaoqaNz https://t.co/YfWOVIjw38\n",
      "RT using #FirstMinuteShopping &amp; #Sweepstakes for a chance to win this sleek Samsonite luggage + other prizes. https://t.co/hNCIutiLMM https://t.co/sawrk6L6h7\n",
      "Shop with the stars. @ShawnCarterSF is auctioning items from artists &amp; athletes &amp; flying one lucky winner to NYC: https://t.co/epZJnlxbZ3 https://t.co/1smxQBZSYS\n"
     ]
    }
   ],
   "source": [
    "top20 = [i['full_text'] for i in sorted(data, key=lambda x: (x['favorite_count'],x['retweet_count']), reverse=True)]\n",
    "for i in top20[:20]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import TypeVar, List, Any\n",
    "T = TypeVar('T', str, List[str])\n",
    "\n",
    "def parse(line: dict, fields: List[T]=['created_at', 'full_text', 'lang', ['user', 'name']]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Parse one twitter data.\n",
    "    -------------\n",
    "    Input:\n",
    "        line(dict): dictionary of that twitter\n",
    "        fields(list): a list of fields to extract. If it's a first-level field, \n",
    "      input str, if it's a nested field, input list of strs.\n",
    "      Default fields = ['created_at', 'full_text', 'lang', ['user', 'name']]\n",
    "    -------------\n",
    "    Return:\n",
    "        a list of values of extracted fields.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for key in fields:\n",
    "        if type(key) == str:\n",
    "            ret.append(line[key])\n",
    "        else:\n",
    "            value = line\n",
    "            for subkey in key:\n",
    "                value = value[subkey]\n",
    "            ret.append(value)\n",
    "    return ret\n",
    "\n",
    "def json2np(json_file: str, fields: List[T]=['created_at', 'full_text', 'lang', ['user', 'name']]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parse json file to np.ndarray.\n",
    "    --------------\n",
    "    Input:\n",
    "        json_file(str): path of input json file\n",
    "        fields(list): fields(list): a list of fields to extract. \n",
    "      If it's a first-level field, input str, if it's a nested field, input list of strs.\n",
    "      Default fields = ['created_at', 'full_text', 'lang', ['user', 'name']]\n",
    "    --------------\n",
    "    Return:\n",
    "        An NxM matrix, where N = lines of json file, M = number of fields\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(json_file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return np.array([parse(line, fields) for line in data])\n",
    "\n",
    "def json2pd(json_file: str, fields: List[T]=['created_at', 'full_text', 'lang', ['user', 'name']]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse json file to pd.DataFrame.\n",
    "    --------------\n",
    "    Input:\n",
    "        json_file(str): path of input json file\n",
    "        fields(list): fields(list): a list of fields to extract. \n",
    "      If it's a first-level field, input str, if it's a nested field, input list of strs.\n",
    "      Default fields = ['created_at', 'full_text', 'lang', ['user', 'name']]\n",
    "    --------------\n",
    "    Return:\n",
    "        An NxM matrix, where N = lines of json file, M = number of fields\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(json_file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame([parse(line, fields) for line in data])\n",
    "    col_names = []\n",
    "    for name in fields:\n",
    "        if type(name) == str:\n",
    "            col_names.append(name)\n",
    "        else:\n",
    "            col_names.append('_'.join(name))\n",
    "    df.columns = [col_names]\n",
    "    return df\n",
    "\n",
    "def json2csv(json_file: str, fields: List[T]=['created_at', 'full_text', 'lang', ['user', 'name']], csv_file: str=None):\n",
    "    \"\"\"\n",
    "    Parse json file and write parsed data to csv file.\n",
    "    --------------\n",
    "    Input:\n",
    "        json_file(str): path of input json file\n",
    "        fields(list): fields(list): a list of fields to extract. \n",
    "      If it's a first-level field, input str, if it's a nested field, input list of strs.\n",
    "      Default fields = ['created_at', 'full_text', 'lang', ['user', 'name']].\n",
    "        csv_file(str): path of output csv fiel. \n",
    "      Default would be the path of json file changing '.json' to '.csv'\n",
    "    \"\"\"\n",
    "    if csv_file == None:\n",
    "        csv_file = json_file[:-5] + '.csv'\n",
    "    df = json2pd(json_file, fields)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Fri Dec 20 00:00:46 +0000 2019',\n",
       "        'CSX CEO Jim Foote joined Governor Ralph Northam today to announce a new landmark agreement to expand reliability and service along Virginiaâ€™s rail lines, creating a pathway to separate passenger and freight operations along the Richmond to D.C. corridor. https://t.co/TZo3CGG12k https://t.co/phvUrJCnQX',\n",
       "        'en', 'CSX'],\n",
       "       ['Thu Dec 19 21:48:16 +0000 2019',\n",
       "        \"CSX announces the appointment of retired Maj. Gen. Suzanne Vautrinot to the companyâ€™s board of directors. Bringing exceptional cybersecurity expertise and leadership skills, Vautrinot will serve on the company's Audit Committee and Governance Committee. \\nhttps://t.co/Iq4zJSGjyY https://t.co/V3qsv3s3a1\",\n",
       "        'en', 'CSX'],\n",
       "       ['Thu Dec 19 01:33:33 +0000 2019',\n",
       "        'CSX participated in the groundbreaking of the rail spur segment at the @nfmipark. Located on a CSX Select Site, the park benefits from our offering of development-ready property. Weâ€™re proud to help drive economic growth in the region. Learn more: https://t.co/mGqYWxBTDu https://t.co/ydn0cEIkYw',\n",
       "        'en', 'CSX'],\n",
       "       ...,\n",
       "       ['Sat Sep 20 14:53:41 +0000 2014',\n",
       "        '#TeamCSX is just making it through Daytona Beach for #cycle2theshore. #BikeMS #CSXCommunity cc: @mssociety http://t.co/q8EsGw0nzK',\n",
       "        'en', 'CSX'],\n",
       "       ['Sat Sep 20 14:05:20 +0000 2014',\n",
       "        \"Who's ready to plant some trees in the Windy City today? http://t.co/LnXSxoDUiz http://t.co/DntcILgeTQ\",\n",
       "        'en', 'CSX'],\n",
       "       ['Fri Sep 19 17:02:37 +0000 2014',\n",
       "        'Shoutout to @MosaicCompany. Thanks for helping growers all over produce better crops. http://t.co/2SoOeEl68G',\n",
       "        'en', 'CSX']], dtype='<U320')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_data = json2np(path+json_files[0])\n",
    "parse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>user_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Fri Dec 20 00:00:46 +0000 2019</td>\n",
       "      <td>CSX CEO Jim Foote joined Governor Ralph Northa...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Thu Dec 19 21:48:16 +0000 2019</td>\n",
       "      <td>CSX announces the appointment of retired Maj. ...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Thu Dec 19 01:33:33 +0000 2019</td>\n",
       "      <td>CSX participated in the groundbreaking of the ...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tue Dec 17 22:27:34 +0000 2019</td>\n",
       "      <td>Automakers trust CSX to get vehicles to market...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Mon Dec 16 20:42:18 +0000 2019</td>\n",
       "      <td>CSX announces the appointment of Jeffery D. Wa...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3229</td>\n",
       "      <td>Sat Sep 20 14:55:46 +0000 2014</td>\n",
       "      <td>RT @uk1fan: Up early getting ready for the #Bi...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>Sat Sep 20 14:54:24 +0000 2014</td>\n",
       "      <td>RT @bomalley: Good luck to #TeamCSX today as t...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3231</td>\n",
       "      <td>Sat Sep 20 14:53:41 +0000 2014</td>\n",
       "      <td>#TeamCSX is just making it through Daytona Bea...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3232</td>\n",
       "      <td>Sat Sep 20 14:05:20 +0000 2014</td>\n",
       "      <td>Who's ready to plant some trees in the Windy C...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3233</td>\n",
       "      <td>Fri Sep 19 17:02:37 +0000 2014</td>\n",
       "      <td>Shoutout to @MosaicCompany. Thanks for helping...</td>\n",
       "      <td>en</td>\n",
       "      <td>CSX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3234 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          created_at  \\\n",
       "0     Fri Dec 20 00:00:46 +0000 2019   \n",
       "1     Thu Dec 19 21:48:16 +0000 2019   \n",
       "2     Thu Dec 19 01:33:33 +0000 2019   \n",
       "3     Tue Dec 17 22:27:34 +0000 2019   \n",
       "4     Mon Dec 16 20:42:18 +0000 2019   \n",
       "...                              ...   \n",
       "3229  Sat Sep 20 14:55:46 +0000 2014   \n",
       "3230  Sat Sep 20 14:54:24 +0000 2014   \n",
       "3231  Sat Sep 20 14:53:41 +0000 2014   \n",
       "3232  Sat Sep 20 14:05:20 +0000 2014   \n",
       "3233  Fri Sep 19 17:02:37 +0000 2014   \n",
       "\n",
       "                                              full_text lang user_name  \n",
       "0     CSX CEO Jim Foote joined Governor Ralph Northa...   en       CSX  \n",
       "1     CSX announces the appointment of retired Maj. ...   en       CSX  \n",
       "2     CSX participated in the groundbreaking of the ...   en       CSX  \n",
       "3     Automakers trust CSX to get vehicles to market...   en       CSX  \n",
       "4     CSX announces the appointment of Jeffery D. Wa...   en       CSX  \n",
       "...                                                 ...  ...       ...  \n",
       "3229  RT @uk1fan: Up early getting ready for the #Bi...   en       CSX  \n",
       "3230  RT @bomalley: Good luck to #TeamCSX today as t...   en       CSX  \n",
       "3231  #TeamCSX is just making it through Daytona Bea...   en       CSX  \n",
       "3232  Who's ready to plant some trees in the Windy C...   en       CSX  \n",
       "3233  Shoutout to @MosaicCompany. Thanks for helping...   en       CSX  \n",
       "\n",
       "[3234 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_data = json2pd(path+json_files[0])\n",
    "parse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse all json files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = './SP-500-Twitter/'\n",
    "json_files = [f for f in os.listdir(json_path) if f.endswith('.json')]\n",
    "csv_path = './SP-500-Twitter-csv/'\n",
    "if not os.path.exists(csv_path):\n",
    "    os.makedirs(csv_path)\n",
    "for json_file in json_files:\n",
    "    json2csv(json_path+json_file, csv_file=csv_path+json_file[:-5]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
